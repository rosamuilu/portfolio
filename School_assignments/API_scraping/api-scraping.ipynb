{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1edc8312",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0953c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Rosa Muilu\"\n",
    "STUDENT_ID = \"14387360\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669fd616",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf49909-80cf-409a-b44d-8553e16effc6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b585cf74037b4c712eb518f3ef8e765",
     "grade": false,
     "grade_id": "cell-3cff1dd74792b356",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Analyzing Gender Distribution Among Scientific Authors in Computational Social Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620fa176-4f89-4b75-96ba-634fc9b8962a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee897c4f558978be6819e4f4f7c261f5",
     "grade": false,
     "grade_id": "cell-8c3327acf8f9bd6f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "*Objective*: Understand the gender distribution of authors across different scientific disciplines using web scraping and API-based gender identification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b39693a-b0d4-4372-a0e0-ecca61ba40c6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c16f3e433bf6dac45bbe3f424b3b397",
     "grade": false,
     "grade_id": "cell-e817d3557fb4f0df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Gender diversity in research is crucial for ensuring diverse perspectives and approaches in scientific inquiry, and for the comprehensiveness and richness of research findings. A balanced gender representation can help challenge systemic biases that might otherwise marginalize or overlook significant areas of study. A diverse research community can also act as a role model, inspiring future generations of all genders to pursue scientific endeavors.\n",
    "\n",
    "This assignment focuses on the question of the gender distribution of researchers in different disciplines, and on identifying how often women are the first or last author of publications. \n",
    "\n",
    "To do so, you will scrape a preprint website, and you will use the API genderize.io to identify the gender of the author based on their name.\n",
    "\n",
    "1. Prepare: Identify a source and decide a scraping strategy\n",
    "\n",
    "2. Scrape the list of articles and authors\n",
    "\n",
    "3. Use API to identify gender \n",
    "\n",
    "4. Analyze gender distribution and authorship order\n",
    "\n",
    "5. Reflect on your findings. \n",
    "\n",
    "6. Scrape the paper abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f155b-74cb-427d-8650-9b76782acb00",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1dda8234fd20fa3e245cee7db24992a9",
     "grade": false,
     "grade_id": "cell-df1834b4ac2ae69b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Setup and requirements\n",
    "First make sure that you have the needed libraries for Python correctly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad6ecfdd-54b5-49ab-b84b-279c4831b89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium\n",
    "# !pip install selenium\n",
    "# !pip install webdriver-manager\n",
    "# !pip install webdriver-manager --upgrade\n",
    "# !pip install packaging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "# driver.get(\"https://www.google.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54b9c67f-2327-4373-9a39-f7b94360b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request\n",
    "#!pip install requests\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acb1d394-4f99-49d2-a25c-edf6af82561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beautifulsoup\n",
    "#!pip install beautifulsoup\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c6550b7-edfc-47df-af69-3d9665090102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa12a9d5-8975-425f-8e13-dc4e586090ea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30d4be4fd67982bfddf22fbe5c9fdf48",
     "grade": false,
     "grade_id": "cell-dcc2e17cee32d989",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1. Plan and strategize\n",
    "\n",
    "We first need to decide which site to scrape and our strategy for doing so. We will focus on a preprint repository. Preprint repositories host and disseminate research papers before they are peer-reviewed and published in academic journals. They therefore give a view of the latest research.\n",
    "\n",
    "There are several repositories that represent different scientific disciplines (e.g., PubMed for life sciences, arXiv for physics and computer science, JSTOR for humanities and social sciences, SocArxiv for social science, etc.) \n",
    "\n",
    "We will here focus on arxiv.org, where many Computational Social Scientists publish, often under the category \"Computers and Society\".\n",
    "\n",
    "You need to pick a page on ArXiv where you can get a representative sample of these research papers -- and which you are allowed to scrape.\n",
    "\n",
    "1. Browse Arxiv.org, and select a page on the website where you can find a sample of research papers.\n",
    "2. Check the robots.txt. Are you allowed to scrape the page you selected? (If not, you will have to choose another one!)\n",
    "3. Decide a strategy for scraping the page as quickly and easily as possible to find the names of the authors for each paper, their titles, and a link to the pages.\n",
    "4. Choose which Python libraries for scraping that you will use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723d96b-af05-42a4-981b-4d8d1722e22d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6b6e1409e86d810ef169596cbabc039",
     "grade": false,
     "grade_id": "cell-4696e716d4f1c81a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 1: Which library is most suitable?\n",
    "\n",
    "Given the structure of the website, which Python libraries for scraping do you think is appropriate to use? Motivate your choice in a few sentences.\n",
    "\n",
    "I'm using Requests with Beautiful Soup, as the site is not dynamic and there is no need to scroll et cetera.\n",
    "\n",
    "[Evaluation: This is an open question. Any motivation that makes sense is fine, but in general, requests make more sense for this page than selenium, since the site in question is not dynamic. Using selenium will be slower and more difficult.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4455efa-4750-4473-aa08-db19ba9bafce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "594b965b0d990ca90076303a53a26d39",
     "grade": false,
     "grade_id": "cell-f043dfd5a37ede8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2. Scrape the list of articles and authors \n",
    "\n",
    "Implement your scraping strategy. Scrape the page and collect the information about the publication. \n",
    "\n",
    "- You will need to get (1) the link to the article, (2) the title of the article, (3) the names of all authors of the paper, in the same order as they appear on the paper. \n",
    "- You need to scrape 200 research papers.\n",
    "\n",
    "- Note that you may need to iterate over multiple pages.\n",
    "- Note that you need to handle possible exceptions and that your code needs to be able to restart if it crashes.\n",
    "- You final result should be a list of dicts, with keys 'title', 'url', and 'authors'. 'authors' should consist of a list where the authors are listed in the order that they were on the paper. \n",
    "- You need to clean and validate your data: check that all papers have authors, that all papers have titles, clean the texts to remove empty spaces and similar, etc.\n",
    "- Store the resulting array persistently as a pickle with the name 'scraping_result.pkl'.\n",
    "\n",
    "For instance: [{'title': 'How to use Large Langauge Models for Text Analysis', 'authors': ['Törnberg, Petter'], 'url':'https://arxiv.org/abs/2307.13106' } ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "data_list = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "base_url = \"https://export.arxiv.org/\"\n",
    "article_list = 'list/cs.CV/'\n",
    "show = 200\n",
    "\n",
    "url = f'{base_url}{article_list}pastweek?skip=0&show={show}'\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    print('done!')\n",
    "else:\n",
    "    print(response)\n",
    "\n",
    "data_list = []\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "article_list_soup = soup.select('dd')\n",
    "\n",
    "for article_soup in article_list_soup:\n",
    "    authors = article_soup.find_all('a')\n",
    "    authors_list = []\n",
    "    for author in authors:\n",
    "        name = author.get_text()\n",
    "        name = name.replace('\\n', '').strip()\n",
    "        name = name.replace('Authors:', '')\n",
    "        authors_list.append(name)\n",
    "    title = article_soup.find('div', {'class': 'list-title mathjax'}).get_text()\n",
    "    article = {\n",
    "        'title': title[8:].strip(),\n",
    "        'authors': authors_list\n",
    "    }\n",
    "    data_list.append(article)\n",
    "\n",
    "identifiers = soup.find_all('a', {'title': 'Abstract'})\n",
    "\n",
    "counter = 0\n",
    "for identifier in identifiers:\n",
    "    identifier = identifiers[counter]['href']\n",
    "    data_list[counter]['url'] = base_url + identifier\n",
    "    counter = counter + 1\n",
    "\n",
    "data = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13393063",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a24a8ecb5c00b7a230f1536e1032db2e",
     "grade": true,
     "grade_id": "cell-eb1c8710173e94df",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check if keys exists in dictionary\n",
    "assert 'title' in data_list[0], \"Key 'title' not found in dictionary\"\n",
    "assert 'authors' in data_list[0], \"Key 'author' not found in dictionary\"\n",
    "assert 'url' in data_list[0], \"Key 'url' not found in dictionary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fb26d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('scraping_result.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scraping_result.pkl', 'rb') as file:\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scaling Laws of Synthetic Images for Model Tra...</td>\n",
       "      <td>[Lijie Fan, Kaifeng Chen, Dilip Krishnan, Dina...</td>\n",
       "      <td>https://export.arxiv.org//abs/2312.04567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gen2Det: Generate to Detect</td>\n",
       "      <td>[Saksham Suri, Fanyi Xiao, Animesh Sinha, Sean...</td>\n",
       "      <td>https://export.arxiv.org//abs/2312.04566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MuRF: Multi-Baseline Radiance Fields</td>\n",
       "      <td>[Haofei Xu, Anpei Chen, Yuedong Chen, Christos...</td>\n",
       "      <td>https://export.arxiv.org//abs/2312.04565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EAGLES: Efficient Accelerated 3D Gaussians wit...</td>\n",
       "      <td>[Sharath Girish, Kamal Gupta, Abhinav Shrivast...</td>\n",
       "      <td>https://export.arxiv.org//abs/2312.04564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Visual Geometry Grounded Deep Structure From M...</td>\n",
       "      <td>[Jianyuan Wang, Nikita Karaev, Christian Ruppr...</td>\n",
       "      <td>https://export.arxiv.org//abs/2312.04563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Scaling Laws of Synthetic Images for Model Tra...   \n",
       "1                        Gen2Det: Generate to Detect   \n",
       "2               MuRF: Multi-Baseline Radiance Fields   \n",
       "3  EAGLES: Efficient Accelerated 3D Gaussians wit...   \n",
       "4  Visual Geometry Grounded Deep Structure From M...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [Lijie Fan, Kaifeng Chen, Dilip Krishnan, Dina...   \n",
       "1  [Saksham Suri, Fanyi Xiao, Animesh Sinha, Sean...   \n",
       "2  [Haofei Xu, Anpei Chen, Yuedong Chen, Christos...   \n",
       "3  [Sharath Girish, Kamal Gupta, Abhinav Shrivast...   \n",
       "4  [Jianyuan Wang, Nikita Karaev, Christian Ruppr...   \n",
       "\n",
       "                                        url  \n",
       "0  https://export.arxiv.org//abs/2312.04567  \n",
       "1  https://export.arxiv.org//abs/2312.04566  \n",
       "2  https://export.arxiv.org//abs/2312.04565  \n",
       "3  https://export.arxiv.org//abs/2312.04564  \n",
       "4  https://export.arxiv.org//abs/2312.04563  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d989cacd-dff0-456e-8967-2dbe08362fc0",
   "metadata": {},
   "source": [
    "## 3. Use Genderize.io to identify author gender\n",
    "\n",
    "The next step is to identify the gender of the authors. To do so, we will use the free API genderdize.io. \n",
    "\n",
    "1. Go to https://genderize.io/ and read the API documnentation.\n",
    "2. Do you need to register to use it? Do you need an API key? \n",
    "3. How do you call the API? What parameters do you need to send? \n",
    "4. What rate limiting is used? How long do you need to wait between calls?\n",
    "\n",
    "You will use what you learned to carry out the following tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d065292a-42e8-4069-8dfd-4d6a1db353fa",
   "metadata": {},
   "source": [
    "#### Task 1: _identify_gender()_\n",
    "Write a function _identify_gender(first_name)_ that takes a name, and uses the API to guess the gender. The function should send a request to genderize.io, and parse the resulting json to a dict. The function should return a dict with the data provided by the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d01c681e-6bce-416d-9c45-03810e9c2376",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8699ffe64f521fe51e152a3c4baae18",
     "grade": false,
     "grade_id": "cell-c02e3ed6f4cf8078",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 14512, 'name': 'Sasha', 'gender': 'female', 'probability': 0.52}\n"
     ]
    }
   ],
   "source": [
    "def identify_gender(first_name):\n",
    "    # YOUR CODE HERE\n",
    "    response = requests.get(f'https://api.genderize.io?name={first_name}')\n",
    "    gender = json.loads(response.text)\n",
    "    return gender\n",
    "# Test\n",
    "print(identify_gender(\"Sasha\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1062326b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d03ef969-2f1e-4ebf-bd1f-3dd0657f2e69",
   "metadata": {},
   "source": [
    "#### Task 2: Identify gender of all authors\n",
    "\n",
    "Your task is now to use your new function to identify the genders of all authors that you previously scraped. \n",
    "\n",
    "To do so, you first need to extract the first name of each author. You need to iterate over these names and use your function to identify the gender of the author.\n",
    "\n",
    "Your result should be a dataframe with the following columns:\n",
    "\n",
    "- article_url | author_full_name | first_name | author_order | estimated_gender | gender_probability\n",
    "\n",
    "Author_order should be a number specifying where the author was in the author list for the publication (e.g., 0 = first author, 1 = second author...) _Estimated_gender_ should contain the API response on gender, and _gender_probability_ the certainty of the gender, according to the API.\n",
    "\n",
    "Note:\n",
    "- You will need to transform your dict to the dataframe shown above, with one author per line. (This means that each URL will be associated to multiple author names.)\n",
    "- Make sure that you respect the rate limiting of the API. \n",
    "- Make sure that you handle exceptions and that your function continues \n",
    "- Note that you get a maximum of 1,000 free calls per day, so you need to make sure that you do not waste your API calls!\n",
    "- The API may not have all names stored. For these names, store a _np.nan_ value as the gender.\n",
    "\n",
    "Pickle the resulting dataframe with the name: 'author_gender.df.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1db8df5b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a2558664a48ddbac7c92c35c0c14e2b",
     "grade": false,
     "grade_id": "cell-b9581c1efe1807cf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>url</th>\n",
       "      <th>first_name</th>\n",
       "      <th>author_order</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scaling Laws of Synthetic Images for Model Tra...</td>\n",
       "      <td>Lijie Fan</td>\n",
       "      <td>https://export.arxiv.org//abs/2312.04567</td>\n",
       "      <td>Lijie</td>\n",
       "      <td>0</td>\n",
       "      <td>{'count': 53, 'name': 'Lijie', 'gender': 'male...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scaling Laws of Synthetic Images for Model Tra...</td>\n",
       "      <td>Kaifeng Chen</td>\n",
       "      <td>https://export.arxiv.org//abs/2312.04567</td>\n",
       "      <td>Kaifeng</td>\n",
       "      <td>1</td>\n",
       "      <td>{'count': 18, 'name': 'Kaifeng', 'gender': 'ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scaling Laws of Synthetic Images for Model Tra...</td>\n",
       "      <td>Dilip Krishnan</td>\n",
       "      <td>https://export.arxiv.org//abs/2312.04567</td>\n",
       "      <td>Dilip</td>\n",
       "      <td>2</td>\n",
       "      <td>{'count': 1956, 'name': 'Dilip', 'gender': 'ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scaling Laws of Synthetic Images for Model Tra...</td>\n",
       "      <td>Dina Katabi</td>\n",
       "      <td>https://export.arxiv.org//abs/2312.04567</td>\n",
       "      <td>Dina</td>\n",
       "      <td>3</td>\n",
       "      <td>{'count': 98305, 'name': 'Dina', 'gender': 'fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Scaling Laws of Synthetic Images for Model Tra...</td>\n",
       "      <td>Phillip Isola</td>\n",
       "      <td>https://export.arxiv.org//abs/2312.04567</td>\n",
       "      <td>Phillip</td>\n",
       "      <td>4</td>\n",
       "      <td>{'count': 106320, 'name': 'Phillip', 'gender':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>AI-SAM: Automatic and Interactive Segment Anyt...</td>\n",
       "      <td>Alison D. Gernand</td>\n",
       "      <td>https://export.arxiv.org//abs/2312.03119</td>\n",
       "      <td>Alison</td>\n",
       "      <td>2</td>\n",
       "      <td>{'error': 'Request limit reached'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>AI-SAM: Automatic and Interactive Segment Anyt...</td>\n",
       "      <td>Jeffery A. Goldstein</td>\n",
       "      <td>https://export.arxiv.org//abs/2312.03119</td>\n",
       "      <td>Jeffery</td>\n",
       "      <td>3</td>\n",
       "      <td>{'error': 'Request limit reached'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>AI-SAM: Automatic and Interactive Segment Anyt...</td>\n",
       "      <td>James Z. Wang</td>\n",
       "      <td>https://export.arxiv.org//abs/2312.03119</td>\n",
       "      <td>James</td>\n",
       "      <td>4</td>\n",
       "      <td>{'error': 'Request limit reached'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>ScAR: Scaling Adversarial Robustness for LiDAR...</td>\n",
       "      <td>Xiaohu Lu</td>\n",
       "      <td>https://export.arxiv.org//abs/2312.03085</td>\n",
       "      <td>Xiaohu</td>\n",
       "      <td>0</td>\n",
       "      <td>{'error': 'Request limit reached'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>ScAR: Scaling Adversarial Robustness for LiDAR...</td>\n",
       "      <td>Hayder Radha</td>\n",
       "      <td>https://export.arxiv.org//abs/2312.03085</td>\n",
       "      <td>Hayder</td>\n",
       "      <td>1</td>\n",
       "      <td>{'error': 'Request limit reached'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1185 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title               authors  \\\n",
       "0     Scaling Laws of Synthetic Images for Model Tra...             Lijie Fan   \n",
       "1     Scaling Laws of Synthetic Images for Model Tra...          Kaifeng Chen   \n",
       "2     Scaling Laws of Synthetic Images for Model Tra...        Dilip Krishnan   \n",
       "3     Scaling Laws of Synthetic Images for Model Tra...           Dina Katabi   \n",
       "4     Scaling Laws of Synthetic Images for Model Tra...         Phillip Isola   \n",
       "...                                                 ...                   ...   \n",
       "1180  AI-SAM: Automatic and Interactive Segment Anyt...     Alison D. Gernand   \n",
       "1181  AI-SAM: Automatic and Interactive Segment Anyt...  Jeffery A. Goldstein   \n",
       "1182  AI-SAM: Automatic and Interactive Segment Anyt...         James Z. Wang   \n",
       "1183  ScAR: Scaling Adversarial Robustness for LiDAR...             Xiaohu Lu   \n",
       "1184  ScAR: Scaling Adversarial Robustness for LiDAR...          Hayder Radha   \n",
       "\n",
       "                                           url first_name  author_order  \\\n",
       "0     https://export.arxiv.org//abs/2312.04567      Lijie             0   \n",
       "1     https://export.arxiv.org//abs/2312.04567    Kaifeng             1   \n",
       "2     https://export.arxiv.org//abs/2312.04567      Dilip             2   \n",
       "3     https://export.arxiv.org//abs/2312.04567       Dina             3   \n",
       "4     https://export.arxiv.org//abs/2312.04567    Phillip             4   \n",
       "...                                        ...        ...           ...   \n",
       "1180  https://export.arxiv.org//abs/2312.03119     Alison             2   \n",
       "1181  https://export.arxiv.org//abs/2312.03119    Jeffery             3   \n",
       "1182  https://export.arxiv.org//abs/2312.03119      James             4   \n",
       "1183  https://export.arxiv.org//abs/2312.03085     Xiaohu             0   \n",
       "1184  https://export.arxiv.org//abs/2312.03085     Hayder             1   \n",
       "\n",
       "                                                 gender  \n",
       "0     {'count': 53, 'name': 'Lijie', 'gender': 'male...  \n",
       "1     {'count': 18, 'name': 'Kaifeng', 'gender': 'ma...  \n",
       "2     {'count': 1956, 'name': 'Dilip', 'gender': 'ma...  \n",
       "3     {'count': 98305, 'name': 'Dina', 'gender': 'fe...  \n",
       "4     {'count': 106320, 'name': 'Phillip', 'gender':...  \n",
       "...                                                 ...  \n",
       "1180                 {'error': 'Request limit reached'}  \n",
       "1181                 {'error': 'Request limit reached'}  \n",
       "1182                 {'error': 'Request limit reached'}  \n",
       "1183                 {'error': 'Request limit reached'}  \n",
       "1184                 {'error': 'Request limit reached'}  \n",
       "\n",
       "[1185 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "from ast import literal_eval\n",
    "\n",
    "# Load your scraped data\n",
    "with open('scraping_result.pkl', 'rb') as f:\n",
    "    data_list = pickle.load(f)\n",
    "\n",
    "df = data_list.explode('authors')\n",
    "df['first_name'] = df['authors'].str.split(' ').str[0]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df['author_order'] = df.groupby('url').cumcount()\n",
    "\n",
    "\n",
    "df['gender'] = df['first_name'].apply(identify_gender)\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f26d3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('gender_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gender_data.pkl', 'rb') as file:\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d515fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rosam\\AppData\\Local\\Temp\\ipykernel_20968\\1516032009.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['gender'][737:] = df['first_name'][737:].apply(identify_gender)\n"
     ]
    }
   ],
   "source": [
    "# request limit reached, completed next day\n",
    "df['gender'][737:] = df['first_name'][737:].apply(identify_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender_probability'] = df['gender'].apply(lambda x: x.get('probability'))\n",
    "df['estimated_gender'] = df['gender'].apply(lambda x: x.get('gender'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8fe3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author_order'] = df.groupby('url').cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = {'url': 'article_url', 'authors': 'author_full_name'}\n",
    "df = df.rename(columns=new_names)\n",
    "df = df.iloc[:, [2, 1, 3, 4, 7, 6]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "000e6bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_url</th>\n",
       "      <th>author_full_name</th>\n",
       "      <th>first_name</th>\n",
       "      <th>author_order</th>\n",
       "      <th>estimated_gender</th>\n",
       "      <th>gender_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://export.arxiv.org//abs/2312.04567</td>\n",
       "      <td>Lijie Fan</td>\n",
       "      <td>Lijie</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://export.arxiv.org//abs/2312.04567</td>\n",
       "      <td>Kaifeng Chen</td>\n",
       "      <td>Kaifeng</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://export.arxiv.org//abs/2312.04567</td>\n",
       "      <td>Dilip Krishnan</td>\n",
       "      <td>Dilip</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://export.arxiv.org//abs/2312.04567</td>\n",
       "      <td>Dina Katabi</td>\n",
       "      <td>Dina</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://export.arxiv.org//abs/2312.04567</td>\n",
       "      <td>Phillip Isola</td>\n",
       "      <td>Phillip</td>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>https://export.arxiv.org//abs/2312.03119</td>\n",
       "      <td>Alison D. Gernand</td>\n",
       "      <td>Alison</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>https://export.arxiv.org//abs/2312.03119</td>\n",
       "      <td>Jeffery A. Goldstein</td>\n",
       "      <td>Jeffery</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>https://export.arxiv.org//abs/2312.03119</td>\n",
       "      <td>James Z. Wang</td>\n",
       "      <td>James</td>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>https://export.arxiv.org//abs/2312.03085</td>\n",
       "      <td>Xiaohu Lu</td>\n",
       "      <td>Xiaohu</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>https://export.arxiv.org//abs/2312.03085</td>\n",
       "      <td>Hayder Radha</td>\n",
       "      <td>Hayder</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1185 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   article_url      author_full_name  \\\n",
       "0     https://export.arxiv.org//abs/2312.04567             Lijie Fan   \n",
       "1     https://export.arxiv.org//abs/2312.04567          Kaifeng Chen   \n",
       "2     https://export.arxiv.org//abs/2312.04567        Dilip Krishnan   \n",
       "3     https://export.arxiv.org//abs/2312.04567           Dina Katabi   \n",
       "4     https://export.arxiv.org//abs/2312.04567         Phillip Isola   \n",
       "...                                        ...                   ...   \n",
       "1180  https://export.arxiv.org//abs/2312.03119     Alison D. Gernand   \n",
       "1181  https://export.arxiv.org//abs/2312.03119  Jeffery A. Goldstein   \n",
       "1182  https://export.arxiv.org//abs/2312.03119         James Z. Wang   \n",
       "1183  https://export.arxiv.org//abs/2312.03085             Xiaohu Lu   \n",
       "1184  https://export.arxiv.org//abs/2312.03085          Hayder Radha   \n",
       "\n",
       "     first_name  author_order estimated_gender  gender_probability  \n",
       "0         Lijie             0             male                0.51  \n",
       "1       Kaifeng             1             male                1.00  \n",
       "2         Dilip             2             male                0.99  \n",
       "3          Dina             3           female                0.99  \n",
       "4       Phillip             4             male                1.00  \n",
       "...         ...           ...              ...                 ...  \n",
       "1180     Alison             2           female                1.00  \n",
       "1181    Jeffery             3             male                1.00  \n",
       "1182      James             4             male                1.00  \n",
       "1183     Xiaohu             0             male                0.98  \n",
       "1184     Hayder             1             male                1.00  \n",
       "\n",
       "[1185 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b002fc50",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94b2428a7e2b93e5c2e81bca2122b796",
     "grade": true,
     "grade_id": "cell-85d226433e71226b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_url</th>\n",
       "      <th>author_full_name</th>\n",
       "      <th>first_name</th>\n",
       "      <th>author_order</th>\n",
       "      <th>estimated_gender</th>\n",
       "      <th>gender_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://export.arxiv.org//abs/2312.04567</td>\n",
       "      <td>Lijie Fan</td>\n",
       "      <td>Lijie</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://export.arxiv.org//abs/2312.04567</td>\n",
       "      <td>Kaifeng Chen</td>\n",
       "      <td>Kaifeng</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://export.arxiv.org//abs/2312.04567</td>\n",
       "      <td>Dilip Krishnan</td>\n",
       "      <td>Dilip</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://export.arxiv.org//abs/2312.04567</td>\n",
       "      <td>Dina Katabi</td>\n",
       "      <td>Dina</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://export.arxiv.org//abs/2312.04567</td>\n",
       "      <td>Phillip Isola</td>\n",
       "      <td>Phillip</td>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://export.arxiv.org//abs/2312.04567</td>\n",
       "      <td>Yonglong Tian</td>\n",
       "      <td>Yonglong</td>\n",
       "      <td>5</td>\n",
       "      <td>male</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://export.arxiv.org//abs/2312.04566</td>\n",
       "      <td>Saksham Suri</td>\n",
       "      <td>Saksham</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://export.arxiv.org//abs/2312.04566</td>\n",
       "      <td>Fanyi Xiao</td>\n",
       "      <td>Fanyi</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://export.arxiv.org//abs/2312.04566</td>\n",
       "      <td>Animesh Sinha</td>\n",
       "      <td>Animesh</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://export.arxiv.org//abs/2312.04566</td>\n",
       "      <td>Sean Chang Culatana</td>\n",
       "      <td>Sean</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                article_url     author_full_name first_name  \\\n",
       "0  https://export.arxiv.org//abs/2312.04567            Lijie Fan      Lijie   \n",
       "1  https://export.arxiv.org//abs/2312.04567         Kaifeng Chen    Kaifeng   \n",
       "2  https://export.arxiv.org//abs/2312.04567       Dilip Krishnan      Dilip   \n",
       "3  https://export.arxiv.org//abs/2312.04567          Dina Katabi       Dina   \n",
       "4  https://export.arxiv.org//abs/2312.04567        Phillip Isola    Phillip   \n",
       "5  https://export.arxiv.org//abs/2312.04567        Yonglong Tian   Yonglong   \n",
       "6  https://export.arxiv.org//abs/2312.04566         Saksham Suri    Saksham   \n",
       "7  https://export.arxiv.org//abs/2312.04566           Fanyi Xiao      Fanyi   \n",
       "8  https://export.arxiv.org//abs/2312.04566        Animesh Sinha    Animesh   \n",
       "9  https://export.arxiv.org//abs/2312.04566  Sean Chang Culatana       Sean   \n",
       "\n",
       "   author_order estimated_gender  gender_probability  \n",
       "0             0             male                0.51  \n",
       "1             1             male                1.00  \n",
       "2             2             male                0.99  \n",
       "3             3           female                0.99  \n",
       "4             4             male                1.00  \n",
       "5             5             male                1.00  \n",
       "6             0             male                1.00  \n",
       "7             1             male                0.53  \n",
       "8             2             male                1.00  \n",
       "9             3             male                1.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert 'article_url' in df.columns, \"article_url column is missing\"\n",
    "assert 'author_full_name' in df.columns, \"author_full_name column is missing\"\n",
    "assert 'first_name' in df.columns, \"first_name column is missing\"\n",
    "assert 'author_order' in df.columns, \"author_order column is missing\"\n",
    "assert 'estimated_gender' in df.columns, \"estimated_gender column is missing\"\n",
    "assert 'gender_probability' in df.columns, \"gender_probability column is missing\"\n",
    "\n",
    "with open('author_gender.df.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04701544-19ce-4f09-b6fc-06d1cae74f58",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f43b38f6f15f75ac827995ace4fe885",
     "grade": true,
     "grade_id": "cell-c2e95f57fe249bfe",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3efaadaf-c7cf-4287-88a4-27b2bdf846e4",
   "metadata": {},
   "source": [
    "## 4. Analyze gender distribution and authorship order\n",
    "\n",
    "Now that you have gathered the necessary data, you will use this data to answer some research questions about gender equality in CSS research. Note that in calculating this, you need to handle that the API may have failed to identify the gender of some authors.\n",
    "\n",
    "1. What fraction of the authors included are women? \n",
    "2. What happens to this fraction if you only include authors for which the gender_probability is higher than 80%? \n",
    "3. Being the first or single author on a research paper is an important status signal among researchers: it often means that you made the most work. What fraction of the first or single authors are women? \n",
    "4. Being the _last_ author on a research paper with _three or more authors_ is also an important status signal: it tends to mean that you were the one to acquire funding or lead the lab. What fraction of the last-authors on papers with three or more author are women?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('author_gender.df.pkl', 'rb') as file:\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6cbbcb4f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e46a8a2f7e1cabdb0061c0d4c9fc86f2",
     "grade": true,
     "grade_id": "cell-3a0cd012a322701a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fraction of women is 0.19929140832595216\n",
      "The fraction of women when including only cases when gender probability is higher than 80 % is 0.14675324675324675\n",
      "The fraction of women in first or single authors is 0.19021739130434784\n",
      "The fraction of women in last authors is 0.1507537688442211\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "gender_df = df[['estimated_gender', 'gender_probability', 'author_order', 'article_url']]\n",
    "gender_df = gender_df.dropna()\n",
    "\n",
    "# 1st question\n",
    "fractions = gender_df['estimated_gender'].value_counts(normalize=True)\n",
    "female_proportion = fractions.get('female')\n",
    "print(f'The fraction of women is {female_proportion}')\n",
    "\n",
    "# 2nd question\n",
    "gender_df_copy = gender_df[gender_df['gender_probability'] > 0.8]\n",
    "fractions = gender_df_copy['estimated_gender'].value_counts(normalize=True)\n",
    "female_proportion = fractions.get('female')\n",
    "print(f'The fraction of women when including only cases when gender probability is higher than 80 % is {female_proportion}')\n",
    "\n",
    "# 3rd question\n",
    "gender_df_first_authors = gender_df[gender_df['author_order'] == 0]\n",
    "fractions = gender_df_first_authors['estimated_gender'].value_counts(normalize=True)\n",
    "female_proportion = fractions.get('female')\n",
    "print(f'The fraction of women in first or single authors is {female_proportion}')\n",
    "\n",
    "# 4th question\n",
    "gender_df_last_authors = gender_df.groupby('article_url')['estimated_gender'].last()\n",
    "fractions = gender_df_last_authors.value_counts(normalize=True)\n",
    "female_proportion = fractions.get('female')\n",
    "print(f'The fraction of women in last authors is {female_proportion}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b370f3-4b58-41f0-b615-5aff1c4a3ab6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66aa822667d179fcd1119f84e5100ee3",
     "grade": false,
     "grade_id": "cell-906708bcf0e226e3",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## 5. Reflect on your findings\n",
    "\n",
    "You have now carried out your analysis of the gender distribution in articles in CSS using scraped data. Reflect on your findings and method, and answer each of the following questions in a few sentences.\n",
    "\n",
    "1. What are the implications of the observed gender distribution and author order in CSS? How do these distributions compare with your expectations?\n",
    "2. How accurate do you think your findings are? What are the limitations of determining gender based solely on names? Are there cultural or regional nuances that the API might miss?\n",
    "3. Reflect on the ethical considerations involved in scraping this data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27044038-f459-45c7-9a7f-d2cd6d15b18d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "253f673902e8de295d8442ad59caa4b8",
     "grade": true,
     "grade_id": "cell-47e469c422287e72",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. It does indicate a male-dominated authorship on a niche field of Computer Science. The difference between women in general and women in first/last positions isn't that big. These distributions match my expectations, but I did hope the difference would not be that great.\n",
    "\n",
    "2. While this kind of drastic spread does say something about the distribution, it is also just the result of 2-3 days of publishing. More datapoints would make the trend clearer. Determining gender based on only first names is not that accurate, as different regions have different gender distributions for names. For example, in Japan, the name 'Mika' is strongly female, but in Finland it is mainly male. Only looking at the first name will not reveal many cultural aspects to gender in different countries. Additionally, just taking the first part of the name and assuming it is the first name will not be accurate in many Asian naming traditions.\n",
    "\n",
    "3. While the site does allow the list data to be scraped, it is ethical to not do it extensively, as it leads to increased bandwith usage and might lead to problems on the website's side. It is also ethical to abide with the website's own guidelines, like using export.arxiv.org mirrorsite instead of the normal site. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d395f6-7186-4239-93db-e3aa75f6abfb",
   "metadata": {},
   "source": [
    "## 6. Scrape the paper abstract\n",
    "\n",
    "Your next task is to get the abstract for each paper. You will use these abstracts in a later exercise in the course, where we will use text analysis to examine whether the content of research papers are a function of the gender of the author. \n",
    "\n",
    "To do so, you need to iterate over the papers that you have already identified, and scrape the abstract from the URL listed. \n",
    "\n",
    "#### Task 1: scrape_abstract()\n",
    "Write a function scrape_abstract(url) that goes to the research paper URL, and scrapes the content of the abstract. The function should return the abstract as a string, and nothing else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a98b5f1-ffd3-4150-a60c-2e738c884956",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea05833922353eeb9ca3fd40d86801b7",
     "grade": false,
     "grade_id": "cell-b8712361e6327b18",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This guide introduces Large Language Models (LLM) as a highly versatile text\n",
      "analysis method within the social sciences. As LLMs are easy-to-use, cheap,\n",
      "fast, and applicable on a broad range of text analysis tasks, ranging from text\n",
      "annotation and classification to sentiment analysis and critical discourse\n",
      "analysis, many scholars believe that LLMs will transform how we do text\n",
      "analysis. This how-to guide is aimed at students and researchers with limited\n",
      "programming experience, and offers a simple introduction to how LLMs can be\n",
      "used for text analysis in your own research project, as well as advice on best\n",
      "practices. We will go through each of the steps of analyzing textual data with\n",
      "LLMs using Python: installing the software, setting up the API, loading the\n",
      "data, developing an analysis prompt, analyzing the text, and validating the\n",
      "results. As an illustrative example, we will use the challenging task of\n",
      "identifying populism in political texts, and show how LLMs move beyond the\n",
      "existing state-of-the-art.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_abstract(url):\n",
    "    \"\"\"\n",
    "    Fetch the abstract from the provided arXiv URL using XPath.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL of the arXiv paper.\n",
    "\n",
    "    Returns:\n",
    "    - str: The abstract of the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    response = requests.get(url)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    abstract = soup.find('blockquote', {'class': 'abstract mathjax'}).get_text()\n",
    "    abstract = abstract[10:]\n",
    "    return abstract\n",
    "# Test\n",
    "url = \"https://export.arxiv.org/abs/2307.13106\"\n",
    "print(scrape_abstract(url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4466e2b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "284598e8243153347beb6d3b0f5691f7",
     "grade": true,
     "grade_id": "cell-862386d3cf70edf2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4783d9a5-b38d-4ccc-9cbe-f2269467e6d8",
   "metadata": {},
   "source": [
    "#### Task 2: Scrape all urls\n",
    "\n",
    "You will now use your function to scrape all the URLs that you collected in step 2.\n",
    "\n",
    "The following will provide instructions for how you can go about this task. However, there are several ways to do this, and you are free to choose your preferred method.\n",
    "\n",
    "Prepare your data:\n",
    "\n",
    "1. Load your list of dicts from step 2 (scraping_result.pkl)\n",
    "2. Use it to create a dataframe. \n",
    "3. Add a column 'scraped' which should be False for all rows, and a column 'abstract' that should be None for all rows.\n",
    "4. Store the dataframe persistently (e.g., by pickling it.)\n",
    "\n",
    "The scraping procedure:\n",
    "\n",
    "1. Load the persitent pickle as dataframe (so that if your computer crashes, the function will continue where you were)\n",
    "2. Repeat the following steps until there are no more rows with scraped == False:\n",
    "3. Fetch a random row with scraped == False\n",
    "4. Go to the URL and scrape the abstract.\n",
    "5. Set abstract column in the dataframe to the resulting abstract, set scraped to True.\n",
    "6. Store the dataframe persistently as a pickle. \n",
    "\n",
    "Remember: \n",
    "- You may use another strategy. However, since you will be scraping many pages, you should expect your scraper to encounter problems along the way. You therefore need to make sure that you regularly store the results persistently.\n",
    "- Make sure to handle any exceptions gracefully.\n",
    "- Be respectful toward the website owners: wait at least one second between each call. \n",
    "\n",
    "Your final result should be a dataframe stored as 'scraped_abstracts.df.pkl', with filled 'abstract' and 'scraped' columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bbacc7-e0a1-4f98-8234-0ef5f76f3489",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<!-- [Evaluation: ]\n",
    "- Load dataframe as df\n",
    "- Check that the len of df = len of the result list from question 2. \n",
    "- Check that each line has an abstract, with len() > 100 e.g.\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scraping_result.pkl', 'rb') as file:\n",
    "    df = pickle.load(file)\n",
    "df['scraped'] = False\n",
    "df['abstract'] = None\n",
    "with open('scraping_process.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'scraping_process.pkl'\n",
    "\n",
    "def process_row(row):\n",
    "    if not row['scraped'] == True:\n",
    "        url = row['url']\n",
    "        abstract = scrape_abstract(url)\n",
    "        row['abstract'] = abstract\n",
    "        row['scraped'] = True\n",
    "    return row\n",
    "\n",
    "df = pd.read_pickle(filepath)\n",
    "df = df.apply(process_row, axis=1)\n",
    "\n",
    "# Rename and save the final dataframe\n",
    "df.to_pickle('scraped_abstracts.df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d95ec0f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd9348c76775a31d0bc7c64630beda0c",
     "grade": true,
     "grade_id": "cell-d2927458cb5edcd5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
